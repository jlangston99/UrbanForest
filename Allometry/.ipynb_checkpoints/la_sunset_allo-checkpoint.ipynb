{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import PatchCollection\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to file\n",
    "home = os.path.expanduser('~')\n",
    "path = f'{home}/UrbanForest/all_clean_LAcounty_sunset.hdf'\n",
    "\n",
    "# read the hdf\n",
    "la = pd.read_hdf(path, key='data')\n",
    "\n",
    "# select desired columns\n",
    "cols=['ID', 'LATITUDE', 'LONGITUDE', 'DBH_LO', 'DBH_HI', 'CREATED',\n",
    "      'UPDATED', 'SOURCE', 'Name_matched', 'Zone']\n",
    "la = la[cols]\n",
    "\n",
    "# drop NAs\n",
    "la.dropna(how='any', axis=0, subset=['DBH_LO', 'DBH_HI'], inplace=True)\n",
    "\n",
    "# capitalize genus names\n",
    "la['Name_matched'] = la.Name_matched.str.capitalize()\n",
    "\n",
    "# convert DBH to cm\n",
    "la['dbh_low']  = 2.54 * la.DBH_LO\n",
    "la['dbh_high'] = 2.54 * la.DBH_HI\n",
    "la.drop(['DBH_LO', 'DBH_HI'], axis=1, inplace=True)\n",
    "\n",
    "# Change date fields to dateTime type\n",
    "la['created'] = pd.to_datetime(la.CREATED)\n",
    "la['updated'] = pd.to_datetime(la.UPDATED)\n",
    "la.drop(['CREATED', 'UPDATED'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will first use allometric equations from :\n",
    "\n",
    " McPherson, E. Gregory; van Doorn, Natalie S.; Peper, Paula J. 2016. Urban tree database.\n",
    " Fort Collins, CO: Forest Service Research Data Archive. Updated 21 January 2020.\n",
    " https://doi.org/10.2737/RDS-2016-0005\n",
    "\n",
    " 'Apps min' and 'Apps max' give the input range (cm) that the authors feel \n",
    "  that the equations are reliable\n",
    " 'InlEmp' and 'SoCalC' are Climate zones where the eqs are different.\n",
    "  SoCalC reference city is Santa Monica, InlEmp is Claremont,\n",
    "  see Table 1, p16 for further Climate zone details.  \n",
    "  \n",
    "  After reading the equations and coefficients, we will get rid of trees that only occur a few times, and trees that we o not have equations for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The equations\n",
    "def mcpherson_eqs():\n",
    "    '''returns dict of equations from table 3 (p24) of McPherson 2020\n",
    "    functions use np so as to be vectorized'''\n",
    "\n",
    "    eq_dict = {'lin'        : (lambda a, b, c, d, e, x, mse: a + b * (x)), \n",
    "                'quad'      : (lambda a, b, c, d, e, x, mse: a + b * x + c * x**2),\n",
    "                'cub'      : (lambda a, b, c, d, e, x, mse: a + b * x + c * x**2 + d * x**3),\n",
    "                'quart'     : (lambda a, b, c, d, e, x, mse:a + b * x + c *x**2 + d * x**3 + e * x**4), \n",
    "                'loglogw1' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1) + (mse/2)))),\n",
    "                'loglogw2' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1)) + (np.sqrt(x) + (mse/2)))),\n",
    "                'loglogw3' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1)) + (x) + (mse/2))),\n",
    "                'loglogw4' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1)) + (x**2) + (mse/2))),\n",
    "                'expow1'    : (lambda a, b, c, d, e, x, mse: np.exp(a+ b * (x) + (mse/2))),\n",
    "                'expow2'    : (lambda a, b, c, d, e, x, mse: np.exp(a + b * (x) + np.sqrt(x) + (mse/2))),\n",
    "                'expow3'    : (lambda a, b, c, d, e, x, mse: np.exp(a + b * (x) + (x) + (mse/2))),\n",
    "                'expow4'    : (lambda a, b, c, d, e, x, mse: np.exp(a + b * (x) + (x**2) + (mse/2)))}\n",
    "\n",
    "    return(eq_dict)\n",
    "\n",
    "eq_dict = mcpherson_eqs()\n",
    "\n",
    "# The cooeficients\n",
    "coef_df = pd.read_csv('TS6_Growth_coefficients.csvx',\n",
    "usecols=['Region', 'Scientific Name', 'Independent variable', 'Predicts component ', 'EqName', 'Units of predicted components',\n",
    "'EqName', 'a', 'b', 'c', 'd', 'e', 'Apps min', 'Apps max'])\n",
    "\n",
    "# Find all the trees with over 100 occurances in the dataset\n",
    "trees = la.Name_matched.value_counts()\n",
    "trees = list(trees.where(trees > 100).dropna().index)\n",
    "\n",
    "# drop trees we do not have equations for\n",
    "trees = [s for s in trees if s in coef_df['Scientific Name'].unique()]\n",
    "la = la.loc[la.Name_matched.isin(trees)]\n",
    "\n",
    "la.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The USGS lidar data is hosted on amazon, so we will nned the AWS client to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl \"https://s3.amazonaws.com/aws-cli/awscli-bundle.zip\" -o \"awscli-bundle.zip\"\n",
    "#!unzip awscli-bundle.zip \n",
    "#!./awscli-bundle/install -b ~/bin/aws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a tmp directory too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a tmp directory\n",
    "\n",
    "!mkdir ~/tmp\n",
    "\n",
    "# make a variable for its path\n",
    "tmp = f'{home}/tmp'\n",
    "\n",
    "# make a variable with the path to aws cli\n",
    "aws = '~/bin/aws'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the top level ept json for the ```USGS_LPC_CA_LosAngeles_2016_LAS_2018``` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "cmd = f'{aws} s3 cp s3://usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json {tmp} --no-sign-request'\n",
    "subprocess.run(cmd, shell=True, capture_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load ```ept.json``` and extract usefull information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'{tmp}/ept.json') as f:\n",
    "    meta = json.load(f)\n",
    "    \n",
    "bounds = meta['bounds']\n",
    "bounds_conf = meta['boundsConforming']\n",
    "srs    = meta['srs']\n",
    "span   = meta['span']\n",
    "schema  = meta['schema']\n",
    "srs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output tells us the data is in EPSG:3857.  There is only a horizontal code present.  Lets reduce the srs to a more sueful form for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = meta['srs']['authority'] + ':' + meta['srs']['horizontal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the srs is EPSG:3857 (Pseudo-Mercator) I don't think it necesaary to reproject.  We do need to use the scale and offset values to find absolute position according to ``read_value * scale + offset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_scale_offset(name, schema):\n",
    "    '''Retruns scale and offset for the spatial dimension given by name'''\n",
    "    for thing in schema:\n",
    "        if thing['name'] == name:\n",
    "            return(thing['scale'], thing['offset'])\n",
    "        \n",
    "x_scale, x_offset = bag_scale_offset('X', schema)\n",
    "y_scale, y_offset = bag_scale_offset('Y', schema)\n",
    "z_scale, z_offset = bag_scale_offset('Z', schema)\n",
    "\n",
    "def rescale(lon, lat, elev=None):\n",
    "    '''Returns point rescaled to the ept coords'''\n",
    "    x = lon * x_scale + x_offset\n",
    "    y = lat * y_scale + y_offset\n",
    "    if elev:\n",
    "        z = elev * z_scale + z_offset\n",
    "        return(x, y, z)\n",
    "    return(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it should be possible to define a bounding box around a tree to query the pointcloud at that local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we will ad 0.00007 degrees in each direction, this is jus a guess based on 5th decimal place ~ 1.1m\n",
    "# also not setting z max and min for the moment\n",
    "def make_scaled_bbox(lat, lon, bounds=None):\n",
    "    '''Returns a bbox in ept coords.\n",
    "    If present bounds is of form [xmin, ymin, zmin, xmax, ymax, zmax]'''\n",
    "    \n",
    "    buf = 0.00007\n",
    "    xmin = lon - buf\n",
    "    ymin = lat - buf\n",
    "    xmax = lon + buf\n",
    "    ymax = lat + buf\n",
    "    xmin, ymin = rescale(xmin, ymin)\n",
    "    xmax, ymax = rescale(xmax, ymax)\n",
    "\n",
    "    # make sure no bbox is out of the ept bbox\n",
    "    if bounds:\n",
    "        xmin = max(xmin, bounds[0])\n",
    "        ymin = max(ymin, bounds[1])\n",
    "        xmax = min(xmax, bounds[3])\n",
    "        ymax = min(ymax, bounds[4])\n",
    "    \n",
    "    return([xmin, xmax], [ymin, ymax])\n",
    "\n",
    "def make_bbox(lat, lon):\n",
    "    buf = 0.00007\n",
    "    xmin = lon - buf\n",
    "    ymin = lat - buf\n",
    "    xmax = lon + buf\n",
    "    ymax = lat + buf\n",
    "    return([xmin, xmax], [ymin, ymax])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pdal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to get the point cloud within the bbox using PDALs ept reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "from string import Template\n",
    "from tqdm import tqdm\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 1986759\n",
    "lat = la.loc[la.ID==id]['LATITUDE'].values[0]\n",
    "lon = la.loc[la.ID==id]['LONGITUDE'].values[0]\n",
    "lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{tmp}/ept.json') as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "scaled_bbox = make_scaled_bbox(lat, lon, bounds=bounds_conf)\n",
    "\n",
    "def bbox_geojson(lat, lon, filename):\n",
    "    '''makes wgs84 bbox as geojson for comparison in gis'''\n",
    "    [xmin, xmax], [ymin, ymax] = make_bbox(lat, lon)\n",
    "    gjson = {'coordinates' : [[[xmin, ymin], [xmin, ymax], [xmax, ymax], [xmax, ymin]]],\n",
    "            'type' : 'Polygon'}\n",
    "    with open(filename, 'w') as of:\n",
    "        json.dump(gjson, of)\n",
    "\n",
    "bbox_geojson(lat, lon, 'xxx.json')        \n",
    "print('geojson written ...')\n",
    "\n",
    "# make and validate pipeline\n",
    "t = Template('''\n",
    "{\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "        \"bounds\": \"([-10425171.940, -10425171.000], [5164494.710, 5164595.200])\",\n",
    "        \"filename\": \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json\",\n",
    "        \"type\": \"readers.ept\",\n",
    "        \"tag\": \"readdata\"\n",
    "        },\n",
    "        {\n",
    "        \"out_srs\": \"EPSG:4326\",\n",
    "        \"type\": \"filters.reprojection\"\n",
    "        },\n",
    "        {\n",
    "        \"filename\": \"xxx.laz\",\n",
    "        \"type\": \"writers.las\"\n",
    "        },\n",
    "        {\n",
    "        \"filename\": \"xxx.tif\",\n",
    "        \"gdalopts\": \"tiled=yes,     compress=deflate\",\n",
    "        \"nodata\": -9999,\n",
    "        \"output_type\": \"idw\",\n",
    "        \"resolution\": 1,\n",
    "        \"type\": \"writers.gdal\",\n",
    "        \"window_size\": 6\n",
    "        }\n",
    "    ]\n",
    "}''')\n",
    "\n",
    "pipe = t.substitute(scaled_bbox=scaled_bbox)\n",
    "print('pipeline json written ...')\n",
    "pipeline = pdal.Pipeline(pipe)\n",
    "print('pipeline string turned into pipeline ...')\n",
    "pipeline.validate()\n",
    "print('pipeline validated ...')\n",
    "\n",
    " # execuite pipeline\n",
    "count = pipeline.execute()\n",
    "print('pipeline executed ...')\n",
    "S = pipeline.arrays[0]\n",
    "metadata = pipeline.metadata\n",
    "log = pipeline.log\n",
    "\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-5c063dddf927>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-5c063dddf927>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    [\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "@delayed\n",
    "def row_bounds_ept_query(i):\n",
    "    # get lat lon of first entry\n",
    "    row = la.iloc[i]\n",
    "    ident = row['ID']\n",
    "    lat = row['LATITUDE']\n",
    "    lon = row['LONGITUDE']\n",
    "\n",
    "    # make bbox in the ept coord system\n",
    "    scaled_bbox = make_scaled_bbox(lat, lon, bounds=bounds_conf)\n",
    "    \n",
    "    # sanity check will raise erros if scalled bbox is not in the ept bounds\n",
    "    assert (scaled_bbox[0][0] > bounds[0]) & (scaled_bbox[0][1] < bounds[3])\n",
    "    assert (scaled_bbox[1][0] > bounds[1]) & (scaled_bbox[1][1] < bounds[4])\n",
    "    \n",
    "    # make and validate pipeline\n",
    "    t = Template('''\n",
    "    [\n",
    "        {\n",
    "            \"type\": \"readers.ept\",\n",
    "            \"filename\": \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json\",\n",
    "            \"bounds\": \"${scaled_bbox}\",\n",
    "            \"resolution\": 1\n",
    "        }\n",
    "    ]''')\n",
    "\n",
    "    pipe = t.substitute(scaled_bbox=tbbox)\n",
    "    pipeline = pdal.Pipeline(pipe)\n",
    "    pipeline.validate()\n",
    "    \n",
    "    # execuite pipeline\n",
    "    count = pipeline.execute()\n",
    "    S = pipeline.arrays[0]\n",
    "    metadata = pipeline.metadata\n",
    "    log = pipeline.log\n",
    "    \n",
    "    # do stuff\n",
    "    sh = S.shape\n",
    "    if sh[0] > 0:\n",
    "        print(f'{ident} is a numpy structured array of shape {sh}.')\n",
    "        return(S)\n",
    "        \n",
    "results = []\n",
    "for i in range(1):\n",
    "    results.append(row_bounds_ept_query(i+10))\n",
    "    \n",
    "with ProgressBar():\n",
    "    S = compute(*results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
