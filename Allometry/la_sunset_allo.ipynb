{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import PatchCollection\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading tree data, allometric equations, etc... ##\n",
    "Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to file\n",
    "home = os.path.expanduser('~')\n",
    "path = f'{home}/UrbanForest/all_clean_LAcounty_sunset.hdf'\n",
    "\n",
    "# read the hdf\n",
    "la = pd.read_hdf(path, key='data')\n",
    "\n",
    "# select desired columns\n",
    "cols=['ID', 'LATITUDE', 'LONGITUDE', 'DBH_LO', 'DBH_HI', 'CREATED',\n",
    "      'UPDATED', 'SOURCE', 'Name_matched', 'Zone']\n",
    "la = la[cols]\n",
    "\n",
    "# drop NAs\n",
    "la.dropna(how='any', axis=0, subset=['DBH_LO', 'DBH_HI'], inplace=True)\n",
    "\n",
    "# capitalize genus names\n",
    "la['Name_matched'] = la.Name_matched.str.capitalize()\n",
    "\n",
    "# convert DBH to cm\n",
    "la['dbh_low']  = 2.54 * la.DBH_LO\n",
    "la['dbh_high'] = 2.54 * la.DBH_HI\n",
    "la.drop(['DBH_LO', 'DBH_HI'], axis=1, inplace=True)\n",
    "\n",
    "# Change date fields to dateTime type\n",
    "la['created'] = pd.to_datetime(la.CREATED)\n",
    "la['updated'] = pd.to_datetime(la.UPDATED)\n",
    "la.drop(['CREATED', 'UPDATED'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will first use allometric equations from :\n",
    "\n",
    " McPherson, E. Gregory; van Doorn, Natalie S.; Peper, Paula J. 2016. Urban tree database.\n",
    " Fort Collins, CO: Forest Service Research Data Archive. Updated 21 January 2020.\n",
    " https://doi.org/10.2737/RDS-2016-0005\n",
    "\n",
    " 'Apps min' and 'Apps max' give the input range (cm) that the authors feel \n",
    "  that the equations are reliable\n",
    " 'InlEmp' and 'SoCalC' are Climate zones where the eqs are different.\n",
    "  SoCalC reference city is Santa Monica, InlEmp is Claremont,\n",
    "  see Table 1, p16 for further Climate zone details.  \n",
    "  \n",
    "  After reading the equations and coefficients, we will get rid of trees that only occur a few times, and trees that we o not have equations for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 664155 entries, 0 to 1089845\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   ID            664155 non-null  int64         \n",
      " 1   LATITUDE      664155 non-null  float64       \n",
      " 2   LONGITUDE     664155 non-null  float64       \n",
      " 3   SOURCE        664155 non-null  object        \n",
      " 4   Name_matched  664155 non-null  object        \n",
      " 5   Zone          663384 non-null  float64       \n",
      " 6   dbh_low       664155 non-null  float64       \n",
      " 7   dbh_high      664155 non-null  float64       \n",
      " 8   created       28472 non-null   datetime64[ns]\n",
      " 9   updated       28472 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(5), int64(1), object(2)\n",
      "memory usage: 55.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# The equations\n",
    "def mcpherson_eqs():\n",
    "    '''returns dict of equations from table 3 (p24) of McPherson 2020\n",
    "    functions use np so as to be vectorized'''\n",
    "\n",
    "    eq_dict = {'lin'        : (lambda a, b, c, d, e, x, mse: a + b * (x)), \n",
    "                'quad'      : (lambda a, b, c, d, e, x, mse: a + b * x + c * x**2),\n",
    "                'cub'      : (lambda a, b, c, d, e, x, mse: a + b * x + c * x**2 + d * x**3),\n",
    "                'quart'     : (lambda a, b, c, d, e, x, mse:a + b * x + c *x**2 + d * x**3 + e * x**4), \n",
    "                'loglogw1' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1) + (mse/2)))),\n",
    "                'loglogw2' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1)) + (np.sqrt(x) + (mse/2)))),\n",
    "                'loglogw3' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1)) + (x) + (mse/2))),\n",
    "                'loglogw4' : (lambda a, b, c, d, e, x, mse: np.exp(a + b * np.log(np.log(x + 1)) + (x**2) + (mse/2))),\n",
    "                'expow1'    : (lambda a, b, c, d, e, x, mse: np.exp(a+ b * (x) + (mse/2))),\n",
    "                'expow2'    : (lambda a, b, c, d, e, x, mse: np.exp(a + b * (x) + np.sqrt(x) + (mse/2))),\n",
    "                'expow3'    : (lambda a, b, c, d, e, x, mse: np.exp(a + b * (x) + (x) + (mse/2))),\n",
    "                'expow4'    : (lambda a, b, c, d, e, x, mse: np.exp(a + b * (x) + (x**2) + (mse/2)))}\n",
    "\n",
    "    return(eq_dict)\n",
    "\n",
    "eq_dict = mcpherson_eqs()\n",
    "\n",
    "# The cooeficients\n",
    "coef_df = pd.read_csv('TS6_Growth_coefficients.csvx',\n",
    "usecols=['Region', 'Scientific Name', 'Independent variable', 'Predicts component ', 'EqName', 'Units of predicted components',\n",
    "'EqName', 'a', 'b', 'c', 'd', 'e', 'Apps min', 'Apps max'])\n",
    "\n",
    "# Find all the trees with over 100 occurances in the dataset\n",
    "trees = la.Name_matched.value_counts()\n",
    "trees = list(trees.where(trees > 100).dropna().index)\n",
    "\n",
    "# drop trees we do not have equations for\n",
    "trees = [s for s in trees if s in coef_df['Scientific Name'].unique()]\n",
    "la = la.loc[la.Name_matched.isin(trees)]\n",
    "\n",
    "la.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Lidar data ## \n",
    "We will be using the ```USGS_LPC_CA_LosAngeles_2016_LAS_2018``` dataset.  The USGS lidar data is hosted in an Amazon S3 bucket, so we will need the AWS client to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl \"https://s3.amazonaws.com/aws-cli/awscli-bundle.zip\" -o \"awscli-bundle.zip\"\n",
    "#!unzip awscli-bundle.zip \n",
    "#!./awscli-bundle/install -b ~/bin/aws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a tmp directory too, if we don't already have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/jovyan/tmp’: File exists\n"
     ]
    }
   ],
   "source": [
    "# make a tmp directory\n",
    "\n",
    "!mkdir ~/tmp\n",
    "\n",
    "# make a variable for its path\n",
    "tmp = f'{home}/tmp'\n",
    "\n",
    "# make a variable with the path to aws cli\n",
    "aws = '~/bin/aws'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the ept metadata ###\n",
    "The top level ept json for the ```USGS_LPC_CA_LosAngeles_2016_LAS_2018``` dataset contains important metadata. We will download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='~/bin/aws s3 cp s3://usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json /home/jovyan/tmp --no-sign-request', returncode=0, stdout=b'Completed 2.4 KiB/2.4 KiB (8.5 KiB/s) with 1 file(s) remaining\\rdownload: s3://usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json to ../tmp/ept.json\\n', stderr=b'')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "cmd = f'{aws} s3 cp s3://usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json {tmp} --no-sign-request'\n",
    "subprocess.run(cmd, shell=True, capture_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load ```ept.json``` and extract usefull information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataType is: laszip\n",
      "hierarchyType is: json\n",
      "srs is:\n",
      "{'authority': 'EPSG', 'horizontal': '3857', 'wkt': 'PROJCS[\"WGS 84 / Pseudo-Mercator\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Mercator_1SP\"],PARAMETER[\"central_meridian\",0],PARAMETER[\"scale_factor\",1],PARAMETER[\"false_easting\",0],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"X\",EAST],AXIS[\"Y\",NORTH],EXTENSION[\"PROJ4\",\"+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs\"],AUTHORITY[\"EPSG\",\"3857\"]]'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(f'{tmp}/ept.json') as f:\n",
    "    meta = json.load(f)\n",
    "    \n",
    "bounds = meta['bounds']\n",
    "bounds_conf = meta['boundsConforming']\n",
    "srs = meta['srs']\n",
    "span   = meta['span']\n",
    "schema  = meta['schema']\n",
    "dataType = meta['dataType']\n",
    "hierarchyType = meta['hierarchyType']\n",
    "\n",
    "def bag_scale_offset(name, schema):\n",
    "    '''Retruns scale and offset for the spatial dimension given by name'''\n",
    "    for thing in schema:\n",
    "        if thing['name'] == name:\n",
    "            return(thing['scale'], thing['offset'])\n",
    "        \n",
    "x_scale, x_offset = bag_scale_offset('X', schema)\n",
    "y_scale, y_offset = bag_scale_offset('Y', schema)\n",
    "z_scale, z_offset = bag_scale_offset('Z', schema)\n",
    "\n",
    "#print some info\n",
    "print(f'dataType is: {dataType}\\nhierarchyType is: {hierarchyType}\\nsrs is:\\n{srs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output tells us the data is in EPSG:3857.  There is only a horizontal code present.  Lets reduce the srs to a more useful form for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EPSG:3857'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srs = meta['srs']['authority'] + ':' + meta['srs']['horizontal']\n",
    "srs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the srs is in Pseudo-Mercator (EPSG:3857), and our data is lat, lon (EPSG:4326) We will have to reproject. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying data ###\n",
    "Now it should be possible to define a bounding box around a tree to query the ept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import osr\n",
    "\n",
    "# For now we will ad 0.00007 degrees in each direction, this is jus a guess based on 5th decimal place ~ 1.1m\n",
    "# also not setting z max and min for the moment\n",
    "def make_scaled_bbox(lat, lon, bounds=None):\n",
    "    '''Returns a bbox in ept coords.\n",
    "    If present bounds is of form [xmin, ymin, zmin, xmax, ymax, zmax]'''\n",
    "    # make the bbox in EPSG:4326\n",
    "    [xmin, xmax], [ymin, ymax] = make_bbox(lat, lon)\n",
    "    \n",
    "    # define transform from EPSG:4326 to EPSG:3857\n",
    "    old_crs = osr.SpatialReference() \n",
    "    old_crs.ImportFromEPSG(4326) \n",
    "    new_crs = osr.SpatialReference() \n",
    "    new_crs.ImportFromEPSG(3857)\n",
    "    transform = osr.CoordinateTransformation(old_crs,new_crs)\n",
    "    \n",
    "    # transform bbox points\n",
    "    xmin, ymin, zmin = transform.TransformPoint(ymin, xmin)\n",
    "    xmax, ymax, zmax = transform.TransformPoint(ymax, xmax)\n",
    "\n",
    "    # TODO:make sure no bbox is out of the ept bbox, for edge cases\n",
    "    # if bounds:\n",
    "        #blah for blah in blah\n",
    "    \n",
    "    return([xmin, xmax], [ymin, ymax])\n",
    "\n",
    "def make_bbox(lat, lon):\n",
    "    buf = 0.00007\n",
    "    xmin = lon - buf\n",
    "    ymin = lat - buf\n",
    "    xmax = lon + buf\n",
    "    ymax = lat + buf\n",
    "    return([xmin, xmax], [ymin, ymax])\n",
    "\n",
    "def bbox_geojson(lat, lon, filename):\n",
    "    '''makes wgs84 bbox as geojson for comparison in gis'''\n",
    "    [xmin, xmax], [ymin, ymax] = make_bbox(lat, lon)\n",
    "    gjson = {'coordinates' : [[[xmin, ymin], [xmin, ymax], [xmax, ymax], [xmax, ymin]]],\n",
    "            'type' : 'Polygon'}\n",
    "    with open(filename, 'w') as of:\n",
    "        json.dump(gjson, of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pdal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to get the point cloud within the bbox using PDALs ept reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdal\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test things out we will select a single tree.  ```id``` is the uniqued ID for the tree in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lat, lon of point from dataset\n",
    "id = 1986759\n",
    "lat = la.loc[la.ID==id]['LATITUDE'].values[0]\n",
    "lon = la.loc[la.ID==id]['LONGITUDE'].values[0]\n",
    "\n",
    "# get bbox coords in EPSG:4326\n",
    "[xmin, xmax], [ymin, ymax] = make_bbox(lat, lon)\n",
    "\n",
    "# make geojson of bbox in EPSG:4326\n",
    "bbox_geojson(lat, lon, 'box_4326.json')\n",
    "\n",
    "# make transformed bbox\n",
    "scaled_bbox = make_scaled_bbox(lat, lon, bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-13143143.136067484, -13143127.551338773],\n",
       " [4005476.7543784794, 4005495.514655751])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_bbox\n",
    "# UTM11 is EPSG:26911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and validate pipeline\n",
    "t = Template('''\n",
    "{\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "        \"bounds\": \"${scaled_bbox}\",\n",
    "        \"filename\": \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json\",\n",
    "        \"type\": \"readers.ept\",\n",
    "        \"tag\": \"readdata\",\n",
    "        \"spatialreference\":\"${srs}\"\n",
    "        },\n",
    "        {\n",
    "        \"type\": \"filters.reprojection\",\n",
    "        \"in_srs\":\"${srs}\",\n",
    "        \"out_srs\": \"EPSG:26911\"\n",
    "        },\n",
    "        {\n",
    "        \"filename\": \"xxx.laz\",\n",
    "        \"type\": \"writers.las\"\n",
    "        },\n",
    "        {\n",
    "        \"filename\": \"xxx.tif\",\n",
    "        \"gdalopts\": \"tiled=yes,     compress=deflate\",\n",
    "        \"nodata\": -9999,\n",
    "        \"output_type\": \"idw\",\n",
    "        \"resolution\": 1,\n",
    "        \"type\": \"writers.gdal\",\n",
    "        \"window_size\": 6\n",
    "        }\n",
    "    ]\n",
    "}''')\n",
    "\n",
    "pipe = t.substitute(scaled_bbox=scaled_bbox, srs=srs, bbox=bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pdal.Pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = pipeline.arrays[0]\n",
    "metadata = pipeline.metadata\n",
    "log = pipeline.log\n",
    "log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the pipline made a DSM for the query box.  We also now have the returns in an np structured array, ```S```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(544,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(0, 2, 0, 1.37839771e+08,  2128, 1, 9370, 515, 1, -8., 0, 1, 0, 401275.33784372, 3743388.626556  ,  9.64),\n",
       "       (0, 2, 0, 1.37839771e+08,  2560, 1, 9370, 515, 1, -7., 0, 1, 0, 401275.13375607, 3743394.60362959,  9.73),\n",
       "       (0, 1, 0, 1.37839771e+08, 11248, 1, 9370, 515, 1, -8., 0, 0, 0, 401276.21260563, 3743387.99682015, 11.06),\n",
       "       (0, 2, 0, 1.37839771e+08,  3120, 1, 9370, 515, 1, -7., 0, 1, 0, 401276.5896368 , 3743390.68247088,  9.68),\n",
       "       (0, 2, 0, 1.37839771e+08,   496, 1, 9370, 515, 1, -7., 0, 1, 0, 401276.21033083, 3743401.41153014,  9.62)],\n",
       "      dtype=[('ClassFlags', 'u1'), ('Classification', 'u1'), ('EdgeOfFlightLine', 'u1'), ('GpsTime', '<f8'), ('Intensity', '<u2'), ('NumberOfReturns', 'u1'), ('OriginId', '<u4'), ('PointSourceId', '<u2'), ('ReturnNumber', 'u1'), ('ScanAngleRank', '<f4'), ('ScanChannel', 'u1'), ('ScanDirectionFlag', 'u1'), ('UserData', 'u1'), ('X', '<f8'), ('Y', '<f8'), ('Z', '<f8')])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a loop of it and parrallelizing it etc...  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed |  0.0s712181 is a numpy structured array of shape (1751,).\n",
      "[########################################] | 100% Completed |  3.1s\n"
     ]
    }
   ],
   "source": [
    "#from tqdm import tqdm\n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "\n",
    "\n",
    "@delayed\n",
    "def row_bounds_ept_query(i):\n",
    "    # get lat lon of first entry\n",
    "    row = la.iloc[i]\n",
    "    ident = row['ID']\n",
    "    lat = row['LATITUDE']\n",
    "    lon = row['LONGITUDE']\n",
    "\n",
    "    # make bbox in the ept coord system\n",
    "    scaled_bbox = make_scaled_bbox(lat, lon, bounds=bounds_conf)\n",
    "    \n",
    "    # sanity check will raise erros if scalled bbox is not in the ept bounds\n",
    "    assert (scaled_bbox[0][0] > bounds[0]) & (scaled_bbox[0][1] < bounds[3])\n",
    "    assert (scaled_bbox[1][0] > bounds[1]) & (scaled_bbox[1][1] < bounds[4])\n",
    "    \n",
    "    # make and validate pipeline\n",
    "    t = Template('''\n",
    "    {\n",
    "    \"pipeline\": [\n",
    "        {\n",
    "        \"bounds\": \"${scaled_bbox}\",\n",
    "        \"filename\": \"https://s3-us-west-2.amazonaws.com/usgs-lidar-public/USGS_LPC_CA_LosAngeles_2016_LAS_2018/ept.json\",\n",
    "        \"type\": \"readers.ept\",\n",
    "        \"tag\": \"readdata\",\n",
    "        \"spatialreference\":\"${srs}\"\n",
    "        },\n",
    "        {\n",
    "        \"type\": \"filters.reprojection\",\n",
    "        \"in_srs\":\"${srs}\",\n",
    "        \"out_srs\": \"EPSG:26911\"\n",
    "        }\n",
    "        ]\n",
    "    }''')\n",
    "\n",
    "    pipe = t.substitute(scaled_bbox=scaled_bbox, srs=srs)\n",
    "    pipeline = pdal.Pipeline(pipe)\n",
    "    pipeline.validate()\n",
    "    \n",
    "    # execuite pipeline\n",
    "    count = pipeline.execute()\n",
    "    S = pipeline.arrays[0]\n",
    "    metadata = pipeline.metadata\n",
    "    log = pipeline.log\n",
    "    \n",
    "    # do stuff\n",
    "    sh = S.shape\n",
    "    if sh[0] > 0:\n",
    "        print(f'{ident} is a numpy structured array of shape {sh}.')\n",
    "        return(S)\n",
    "        \n",
    "results = []\n",
    "for i in range(1):\n",
    "    results.append(row_bounds_ept_query(i+10))\n",
    "    \n",
    "with ProgressBar():\n",
    "    S = compute(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
